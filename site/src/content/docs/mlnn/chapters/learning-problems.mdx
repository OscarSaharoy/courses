---
title: Learning Problems
---
import Aside from '@components/Aside/Aside.tsx';
import Comments from '@components/Comments/Comments.tsx';
import { Tabs, TabItem, LinkButton } from '@astrojs/starlight/components';

<div style='display: flex; justify-content: space-around;'>
	<LinkButton target="_blank" href="/courses/notes/mlnn/learning-problems.pdf" variant="secondary" icon="document" >Download</LinkButton>
	<LinkButton target="_blank" href="/courses/notes/mlnn/master.pdf" variant="primary" icon="open-book" >Download</LinkButton>
</div>

<div style='text-align: center'><em>
	We now aim to combine the concepts which we have thus far encountered, and construct a formal definition for a learning problem. We will begin by defining the necessary inputs, and the desired outputs, before working towards the fundamental definition of PAC learnability, which will allow us to have some notion of the possibility of learning a certain problem.
</em></div>

## Introduction
We begin the chapter by laying out an example which we will reference and build around throughout.

Suppose that you are a loaner, aiming to determine if an arbitrary loanee will or will not default on the loan you give them. We phrase the problem deliberately in this manner, to emphasize the simplistic scenario in which we are working -- the value of the loan is predetermined, and the goal is to provide a binary classification of loanees.

What information will be useful in this task? How will we determine if our solution is adequate? How will be provide a useful notation to describe the problem?

## A formal learning model :badge[Basic]{variant=success}
### Learning input
The learner of a statistical learning problem has access to the following,

	- Domain set: an arbitrary set which we typically label by $ \mathcal{X} $. This set is nothing more than the objects to which we aim to assign labels. In the example outlined in the previous section, this set $ \mathcal{X} $ is the set of potential loanees.
	- Label set: A set which we typically label by $ \mathcal{Y} $. This is the set of possible labels, e.g., $ \{ \text{ default } , \text{ no default } \} $. Of course, it is more common to denote the possible labels simply by integers, and in our binary classification example, we will take $ \mathcal{Y}=\{ \pm1 \} $.
	- Training data: a finite sequence of pairs $ ( x_{i}, y_{i} )\in \mathcal{X}\times \mathcal{Y} $, i.e., a sequence of labelled objects. In our case, this will be a sequence of past loaners together with information relating to their repayment. We will tend to denote the training data by $ \mathcal{S} = ( ( x_{1}, y_{1} ), ..., ( x_{m}, y_{m} ) ) $, where $ m $ is the size of the training data.


<Aside type='comment' title='Remark' name=''>
	It is important to note that the training data is described as a sequence rather than a set. This is a rather nuanced point, and not one which we will dedicate much thought to. The reason for such pedanticity is that there exist learning algorithms which are dependent on the order of the training points, and there is no necessity for $ \mathcal{S} $ to be duplicate free.

	It is however still common to refer to $ \mathcal{S} $ as the training set.
</Aside>

### Learning output

	- Prediction rule: the only output from a statistical learning problem is a function $ h: \mathcal{X} \to \mathcal{Y} $. We also refer to the function as a hypothesis, and is a rule which the learner can use to label new elements of the domain space.

	      If we are considering an algorithm $ \mathcal{A} $, then we will denote by $ \mathcal{A}( \mathcal{S} ) $ or $ h_{\mathcal{S}} $ the output hypothesis of the algorithm.


### A data generating model
It is important to consider how exactly the training data is generated, as this is integral to how we consider the learning to occur. We assume that there is some probability distribution $ \mathcal{D} $ over $ \mathcal{X} $ which, importantly, the learner does not know about. For the time being, we also make the assumption that there is some 'correct' labelling function $ f: \mathcal{X} \to \mathcal{Y}: x_{i} \mapsto y_{i} $, which maps the object $ x_{i} $ to it's correct label in every case. Explicitly,
$$
\begin{align*}
	f ( x_{i} ) = y_{i} \quad \forall i.
\end{align*}
$$

In every learning scenario, it is exactly this function $ f $ which the learner aims to obtain. In our example, we can suppose that we have past data, $ \mathcal{S} $ which provides a source of truth in the way that each of the training examples is an individual of whom we <em> know </em> the loaning outcome.


### Measure of success
The final element of our formal learning model is the means to quantify the success of our learning, or more strictly our hypothesis, $ h $. In our classification problem, the error of the hypothesis $ h $ is defined to be the probability that for a given data point $ x \in \mathcal{X} $, the predicted classification is not the same as the 'correct' classification. Notatively,
$$
\begin{align*}
	L_{\mathcal{D}, f}( h ) := \mathbb{P}_{x \sim \mathcal{D}}\left[ h ( x ) \neq f ( x ) \right]
\end{align*}
$$

<Aside type='comment' title='Remark' name=''>
	The notation of $ L_{\mathcal{D}, f} $ is well chosen since it makes clear the dependence of the error of the classifier on the underlying distribution $ \mathcal{D} $ and correct classification function $ f $, although if the context is clear we will omit this for brevity.

	It is also common to refer to the probability $ L_{\mathcal{D}, f}( h ) $ as the 'risk', and we will often refer to it as such.
</Aside>

## Empirical Risk Minimisation :badge[Intermediate]{variant=caution}
We now have a good idea about the inputs, outputs and measures of success of a formal learning problem. The natural next step is to explore the strategy by which we can solve such problems.

It maybe initially seems obvious that we would like to minimise the risk, explicitly reducing the probability of incorrect classification, implicitly bringing our hypothesis output $ h $ closer to the true classifier $ f $. Although this is the right idea, we are the learner is limited by their knowledge of the problem. In particular, the learning knows nothing of the distribution $ \mathcal{D} $, nothing of the true classifier $ f $, and hence nothing of the risk $ L_{\mathcal{D}, f} $. As a result, we must construct a different quantity which we would like to minimise.

<Aside type='definition' title='Definition' name='[Empirical risk]'>
	Given a training sequence $ \mathcal{S} = ( ( x_{1}, y_{1} ), ..., ( x_{m}, y_{m} ) ) $, the <em> empirical risk </em> with respect to $ \mathcal{S} $ is defined and denoted by,
	$$
\begin{align*}
		L_{\mathcal{S}}( h ) := \frac{|\left\{ i \in \{ 1,...,m \}: h ( x_{i} ) \neq f ( x_{i} ) \right\}|}{m}
	\end{align*}
$$
</Aside>

This definition is basic in spite of it's verbosity. The numerator is expressing the process of counting misclassifications on the training set, and we are dividing by the size of the training set to ensure that $ L_{\mathcal{S}} $ is bounded between $ 0 $ and $ 1 $.

The empirical risk is a good quantity to consider in learning problems since it is related to the training data available to us, i.e., ''the snapshot of the world available to the learner''[^schwartz]. So, our strategy of learning comes down to minimising the empirical risk -- finding a hypothesis $ h $, which agrees well with $ f $ on the training set. Learning by this strategy is called <em> empirical risk minimisation </em>, referred to by $ \mathrm{ERM} $.

### Overfitting
We now explore a pitfall into which many learning algorithms fall, and one which we will illustrate using the framework so far developed in this chapter, working again with the example of loaning.

<Aside type='example' title='Example' name=''>
	In order to illustrate overfitting in the $ \mathrm{ERM} $ paradigm, we suppose that the underlying distribution $ \mathcal{D} $ on loan applicants $ \mathcal{X} $ is uniform, that is, the correct classification of the feature space is half-half between default ($ 1 $) and non-default ($ -1 $).

	It may seem initially that the process of finding an $ \mathrm{ERM} $ hypothesis is challenging, but this is not actually the case. We can find a scenario agnostic $ \mathrm{ERM} $ hypothesis,
	$$
\begin{align*}
		h_{\mathcal{S}}( x ) = \begin{cases}
			                       y_{i} & \text{ if } \exists i \in \{ 1,...,m \} \text{ s.t. } x_{i} = x \\
			                       1     & \text{ otherwise }
		                       \end{cases}
	\end{align*}
$$

	What exactly is this hypothesis doing? If the selected data point $ x \in \mathcal{X} $ can be found in the training set $ \mathcal{S} $, then the hypothesis returns the label which is assigned to $ x $ in the training set. Otherwise, the hypothesis returns $ 1 $, i.e., predicts that the loanee will be good for their loan and won't default. This hypothesis $ h_{\mathcal{S}} $ is $ \mathrm{ERM} $ since is agrees exactly with the labels provided in the dataset, and hence has $ L_{\mathcal{S}}( h_{\mathcal{S}} ) = 0 $ -- there is zero probability of the hypothesis incorrectly classifying an instance of the training set. However, if the hypothesis returns $ -1 $ on only a finite number of instances, then $ L_{\mathcal{D}, f}( h_{\mathcal{S}} ) = 1/2 $.

	We have found a hypothesis which according to our strategy provides perfect learning, but in reality, performs very badly. That is, the hypothesis cannot generalise. It is 'overfitted' to the training set.
</Aside>

The previous example outlines a clear failure of the $ \mathrm{ERM} $ paradigm, but we shouldn't let this provide obstruction to it's use in total; instead we will find ways to reprimand it's issues.

## Limiting the hypothesis space
One way to address the problem of overfitting in $ \mathrm{ERM} $ is to limit the possible hypotheses which we will output from learning. In particular, the learner should explicitly define a set of possible predictors. We call this set the <em> hypothesis class </em>, and typically label it by $ \mathcal{H} $, which contains hypotheses of the form we have seen previously; functions $ h: \mathcal{X} \to \mathcal{Y} $.

In order to distinguish the $ \mathrm{ERM} $ learning rule from the hypothesis restricted analogue, we use $ \mathrm{ERM}_{\mathcal{H}} $. Furthermore, we can understand the $ \mathrm{ERM}_{\mathcal{H}} $ learning rule as minimisation of the empirical risk over the set of hypotheses. That is,
$$
\begin{align*}
	\mathrm{ERM}_{\mathcal{H}}( \mathcal{S} ) \in \argmin_{h \in \mathcal{H}}\{ L_{\mathcal{S}}( h ) \}.
\end{align*}
$$

<Aside type='comment' title='Notation' name=''>
	For those who haven't seen it before, the notation $ \argmin_{x \in \mathbb{R}}\{ f ( x ) \} $ denotes the value (or set of values) of $ x $ in the set $ \mathbb{R} $ which minimises the function $ f $. So, in our case, we are looking for the hypothesis $ h $ in the hypothesis set $ \mathcal{H} $ which minimises the empirical risk function $ L_{\mathcal{S}} $.
</Aside>

It is natural to ask how exactly a learner should go about limiting/defining a hypothesis set. The restriction to a certain set of hypotheses should be based on some prior knowledge of the problem, the process of choosing this hypothesis set is referred to as <em> inductive bias </em>. It should also be fully considered whether or not the restricted hypothesis set is a good one; will the restriction actually prevent overfitting of the learner? We will explore some examples in this direction later on.

<Aside type='example' title='Example' name=''>
	A natural way to limit the potential hypotheses is to limit the size of the hypothesis set. That is, we may impose a finiteness condition on $ \mathcal{H} $. We could let $ \mathcal{H} $ be the set of all predictors which can be implemented via python programs of at most $ 10^{32} $ characters. There are a number of other common examples.
</Aside>

### Assumptions on the learning problem
We are limiting the hypothesis space in order to prevent overfitting, but how can we be sure that this doesn't limit our ability to learn the problem. That is, how can we be sure that there is an $ \mathrm{ERM} $ hypothesis $ h $ contained in the limited space $ \mathcal{H} $. The short answer is that we can't, but we actually only want to consider situations where this is the case.

<Aside type='definition' title='Definition' name='[Realisability]'>
	The <em> realisability assumption </em> is the assumption that there exists $ h ^{*} \in \mathcal{H} $ such that $ L_{\mathcal{D}, f}( h ^{*} ) = 0 $. In particular, our hypothesis set $ \mathcal{H} $ contains a hypothesis which minimises the true risk.
</Aside>

Under the realisability assumption, we know that $ L_{\mathcal{S}}( h ^{*} ) = 0 $, and furthermore know that every $ \mathrm{ERM} $ hypothesis is also such that $ L_{\mathcal{S}}( h_{\mathcal{S}} ) = 0 $.

It is also important to mention an assumption we make on the training set $ \mathcal{S} $. Since the learner only has access to $ \mathcal{S} $, and we are interested in the true risk of the hypothesis $ h $, we must consider the relationship between $ \mathcal{S} $ and $ \mathcal{D} $.

<Aside type='definition' title='Definition' name='[Independent and identically distributed]'>
	The <em> i.i.d. assumption </em> is the assumption that the training set $ \mathcal{S} $ is constructed as a sequence of independent, and identically distributed examples according to the true distribution $ \mathcal{D} $. We denote this relationship by $ \mathcal{S} \sim \mathcal{D}^{m} $.
</Aside>

<Aside type='comment' title='Remark' name=''>
	It is a useful exercise to track the randomness of our process.
	
		- The training set $ \mathcal{S} \sim \mathcal{D}^{m} $ is an $ m $-collection of randomly chosen i.i.d. examples labelled by $ f $.
		- The hypothesis $ h_{\mathcal{S}} $ inherits this randomness since it is selected according to it's minimisation of $ L_{\mathcal{S}} $.
		- The true risk of this hypothesis, $ L_{\mathcal{D}, f}( h_{\mathcal{S}} ) $ inherits this randomness from $ h_{\mathcal{S}} $.
	

	It is not reasonable to expect with certainty that the training sample $ \mathcal{S} $ will be representative of $ \mathcal{D} $, and as such we introduce a parameter $ \delta $, to quantify this uncertainty. Explicitly, $ \delta $ denotes the probability of attaining a non-representative sample $ \mathcal{S} $ from $ \mathcal{D} $.
</Aside>

## PAC Learnability :badge[Challenging]{variant=danger}
<Aside type='definition' title='Definition' name='[PAC learnability]'>
	A hypothesis class $ \mathcal{H} $ is said to be <em> Probably Approximately Correct (PAC) learnable </em> if there exists a function $ m_{\mathcal{H}}: ( 0,1 )^{2}\to \mathbb{N} $ and a learning algorithm $ \mathcal{A} $ that satisfy the following condition.

	For every $ \epsilon, \delta \in ( 0,1 ) $, for every probability distribution $ \mathcal{D} $ over $ \mathcal{X} $, and for every labelling function $ f: \mathcal{X}\to \mathcal{Y} $, under the assumption of realisability, and with $ m \geq m_{\mathcal{H}}( \epsilon, \delta ) $,
	$$
\begin{align*}
		\mathbb{P}_{\mathcal{S}\sim \mathcal{D}^{m}}\left[ L_{\mathcal{D}, f}( h_{\mathcal{S}} )\leq \epsilon \right] \geq 1-\delta
	\end{align*}
$$
</Aside>

<Aside type='comment' title='Remark' name=''>
	If you haven't taken any higher level mathematics courses, it can be said, almost with certainty, that this is the most intimidating definition you have seen. The majority of the intimidation is down to (necessarily) verbose notation. The idea hidden in the definition is however fairly intuitive.

	If we think only of the name of the definition, we can gain some insight as to what we are trying to describe.
	
		- <b> Correct </b>: the algorithm returns a hypothesis which is correct. That is, the risk of $ h_{\mathcal{S}} $ is zero.
		- <b> Approximately Correct </b>: the algorithm returns a hypothesis which is correct within some bound. That is, the risk of $ h_{\mathcal{S}} $ is less than some approximation parameter $ \epsilon $.
		- <b> Probably Approximately Correct </b>: the algorithm returns a hypothesis which is approximately correct with probability $ 1-\delta $ where $ \delta $ is a parameter quantifiying the probability.
	

	So, as we make the restrictions on the quality of the returned hypothesis stricter, i.e., we decrease $ \epsilon $ to ensure that the hypothesis performs better on the problem, and we decrease $ \delta $ to ensure that we obtain a suitably learned hypothesis with greater probability, we have a function $ m_{\mathcal{H}} $ which will tell us how <em> large </em> our training data set $ \mathcal{S} $ must be, in order to learn the problem according to our restrictions.
</Aside>

<Aside type='result' title='Proposition' name=''>
	Every finite hypothesis class $ \mathcal{H} $ is $ \mathrm{PAC} $ learnable.
</Aside>

## Exercises
### Exercises :badge[Basic]{variant=success}
<Tabs>
	<TabItem label='Problem'>
	What are the elements of a statistical learning problem which the learner has access to?
	</TabItem>
	<TabItem label='Solution'>
		There are three elements of the problem which the learner has access to,
		
			- Domain set
			- Label set
			- Training data
		

		You could make an argument that the training data which is available to the learner is representative of the access which the learner has to the domain set $ \mathcal{X} $. If the learner had full access to $ \mathcal{X} $, learning by memorisation would be a successful method.
	</TabItem>
</Tabs>

### Exercises :badge[Intermediate]{variant=caution}
### Exercises :badge[Challenging]{variant=danger}

<div style="margin-top: 10rem">
[^schwartz]: <em> Understanding Machine Learning - from theory to algorithms </em> -- Shalev-Schwartz and Ben-David
</div>